# bsprc_net_training (version 0.0.0)

몬테카를로 시뮬레이션을 이용해 임의의 모수, 옵션 변수에 대해 대량의 옵션의 시간가치(=price-payoff) 데이터를 생성하고, 
이후 신경망으로 이 데이터를 최대한 잘 학습하여 신경망이 사실상 옵션의 closed pricing formula를 대신하게 하는 것을 목표로 합니다. 
현재 구현은 데이터 생성은 cpu, 신경망 학습은 gpu로 진행하며, cpu의 병렬 처리 능력을 보완하기 위해 10개의 멀티프로세스가 돌아갑니다. 나중에 데이터 생성도 gpu 기반으로 바꿀 계획입니다.
아직 Black-Scholes model의 vanilla option에만 개발이 이루어져있으나, 나중에는 임의의 모델과 임의의 상품에 대해 코드가 돌아가도록 확장할 계획입니다.

중요한 노트북(.ipynb) 파일은 다음과 같습니다.
1. sample.ipynb: 데이터 생성
2. train.ipynb: 신경망 모형 학습
3. test.ipynb: 신경망 성능 테스트

이외의 파이썬(.py) 파일은 다음과 같습니다.
1. model.py: 신경망 정의
2. utils.py: 다양한 유틸리티 함수 정의

또한, 두 개의 폴더가 있으며 역할은 다음과 같습니다.
1. data: 생성된 데이터가 저장되며 data/train에는 훈련용 파일이 data/test에는 테스트용 파일이 저장됩니다.
2. net: 학습된 신경망이 저장됩니다. 신경망의 이름에는 훈련에 사용한 데이터의 개수가 포함되어 있습니다.

이외에도 다음과 같은 사실이 중요합니다.
a. 각 데이터 파일은 10,000개의 값을 가지고 있습니다.
b. 모수와 옵션 변수의 범위는, 변동성 sigma는 0.01에서 1까지, 만기 T는 0.01에서 1까지, 행사가 K는 log(K)/sqrt(T)가 -2에서 2까지 되게끔 설정되어 있습니다.
c. 신경망은 노드 1000개의 히든 레이어 2개를 가지고, sigma, T, K를 받아들여, 옵션의 시간가치 tv를 내놓습니다.
d. 신경망은 ADAM 기반으로 기반으로 학습되며, 학습 데이터 중 learning rate decay 검사를 위한 30%는 검증용으로 사용됩니다.
e. 검증용 데이터셋의 loss가 더이상 떨어지지 않는다면 early stopping rule이 적용됩니다. (언제 학습을 종료할까는 매우 중요한 문제입니다.. 후에 개선이 필요합니다.)
f. 만약 정말로 실험이 잘 되었다면, 데이터 개수를 10배씩 늘릴 때, test.ipynb의 R2는 대략 10배씩 증가하고, MSE는 대략 10배씩 감소될 것이 기대됩니다.

## 사용법
1. sample.ipynb : 데이터 생성
한개당 10,000개의 값이 저장된다는 사실을 기억하세요.
* train_data_num : 훈련용 데이터 파일의 개수
* test_data_num : 테스트용 데이터 파일의 개수
2. train.ipynb : 신경망 모형 학습
* data_num : 몇 개의 데이터를 사용하여 신경망을 훈련할지 정하세요.
  - 10,000의 배수여야 합니다.
3. test.ipynb : 신경망 성능 테스트
* train_data_num : 얼만큼 데이터를 사용해 학습된 신경망을 테스트 할 것인지 정하세요.
  - 10,000의 배수여야 합니다.
* test_data_num : 테스트 데이터의 개수를 정하세요. 왠만하면 생성한 테스트용 데이터를 모두 사용하세요.
  - 10,000의 배수여야 합니다.
